<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>diffsynth.models.sd3_dit &mdash; DiffSynth-Studio 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=01f34227"></script>
        <script src="../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            DiffSynth-Studio
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"></div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">DiffSynth-Studio</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">diffsynth.models.sd3_dit</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for diffsynth.models.sd3_dit</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">einops</span> <span class="kn">import</span> <span class="n">rearrange</span>
<span class="kn">from</span> <span class="nn">.svd_unet</span> <span class="kn">import</span> <span class="n">TemporalTimesteps</span>
<span class="kn">from</span> <span class="nn">.tiler</span> <span class="kn">import</span> <span class="n">TileWorker</span>



<div class="viewcode-block" id="PatchEmbed">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.PatchEmbed">[docs]</a>
<span class="k">class</span> <span class="nc">PatchEmbed</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">1536</span><span class="p">,</span> <span class="n">pos_embed_max_size</span><span class="o">=</span><span class="mi">192</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed_max_size</span> <span class="o">=</span> <span class="n">pos_embed_max_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="n">patch_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed_max_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed_max_size</span><span class="p">,</span> <span class="mi">1536</span><span class="p">))</span>

<div class="viewcode-block" id="PatchEmbed.cropped_pos_embed">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.PatchEmbed.cropped_pos_embed">[docs]</a>
    <span class="k">def</span> <span class="nf">cropped_pos_embed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">):</span>
        <span class="n">height</span> <span class="o">=</span> <span class="n">height</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span>
        <span class="n">width</span> <span class="o">=</span> <span class="n">width</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span>
        <span class="n">top</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_embed_max_size</span> <span class="o">-</span> <span class="n">height</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">left</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_embed_max_size</span> <span class="o">-</span> <span class="n">width</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">spatial_pos_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">[:,</span> <span class="n">top</span> <span class="p">:</span> <span class="n">top</span> <span class="o">+</span> <span class="n">height</span><span class="p">,</span> <span class="n">left</span> <span class="p">:</span> <span class="n">left</span> <span class="o">+</span> <span class="n">width</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">spatial_pos_embed</span></div>


<div class="viewcode-block" id="PatchEmbed.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.PatchEmbed.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent</span><span class="p">):</span>
        <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">latent</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">latent</span><span class="p">)</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="n">latent</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">pos_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cropped_pos_embed</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">latent</span> <span class="o">+</span> <span class="n">pos_embed</span></div>
</div>




<div class="viewcode-block" id="TimestepEmbeddings">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.TimestepEmbeddings">[docs]</a>
<span class="k">class</span> <span class="nc">TimestepEmbeddings</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_proj</span> <span class="o">=</span> <span class="n">TemporalTimesteps</span><span class="p">(</span><span class="n">num_channels</span><span class="o">=</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">flip_sin_to_cos</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">downscale_freq_shift</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timestep_embedder</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">)</span>
        <span class="p">)</span>

<div class="viewcode-block" id="TimestepEmbeddings.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.TimestepEmbeddings.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">time_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_proj</span><span class="p">(</span><span class="n">timestep</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">time_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestep_embedder</span><span class="p">(</span><span class="n">time_emb</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">time_emb</span></div>
</div>




<div class="viewcode-block" id="AdaLayerNorm">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.AdaLayerNorm">[docs]</a>
<span class="k">class</span> <span class="nc">AdaLayerNorm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">single</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">single</span> <span class="o">=</span> <span class="n">single</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="k">if</span> <span class="n">single</span> <span class="k">else</span> <span class="mi">6</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<div class="viewcode-block" id="AdaLayerNorm.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.AdaLayerNorm.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">emb</span><span class="p">):</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="n">emb</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">single</span><span class="p">:</span>
            <span class="n">scale</span><span class="p">,</span> <span class="n">shift</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale</span><span class="p">)</span> <span class="o">+</span> <span class="n">shift</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shift_msa</span><span class="p">,</span> <span class="n">scale_msa</span><span class="p">,</span> <span class="n">gate_msa</span><span class="p">,</span> <span class="n">shift_mlp</span><span class="p">,</span> <span class="n">scale_mlp</span><span class="p">,</span> <span class="n">gate_mlp</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale_msa</span><span class="p">)</span> <span class="o">+</span> <span class="n">shift_msa</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">gate_msa</span><span class="p">,</span> <span class="n">shift_mlp</span><span class="p">,</span> <span class="n">scale_mlp</span><span class="p">,</span> <span class="n">gate_mlp</span></div>
</div>




<div class="viewcode-block" id="JointAttention">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.JointAttention">[docs]</a>
<span class="k">class</span> <span class="nc">JointAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_a</span><span class="p">,</span> <span class="n">dim_b</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">only_out_a</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">only_out_a</span> <span class="o">=</span> <span class="n">only_out_a</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">a_to_qkv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_a</span><span class="p">,</span> <span class="n">dim_a</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b_to_qkv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_b</span><span class="p">,</span> <span class="n">dim_b</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">a_to_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_a</span><span class="p">,</span> <span class="n">dim_a</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">only_out_a</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b_to_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_b</span><span class="p">,</span> <span class="n">dim_b</span><span class="p">)</span>

<div class="viewcode-block" id="JointAttention.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.JointAttention.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states_a</span><span class="p">,</span> <span class="n">hidden_states_b</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states_a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">qkv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">a_to_qkv</span><span class="p">(</span><span class="n">hidden_states_a</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_to_qkv</span><span class="p">(</span><span class="n">hidden_states_b</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">hidden_states_a</span><span class="p">,</span> <span class="n">hidden_states_b</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:</span><span class="n">hidden_states_a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="n">hidden_states_a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:]</span>
        <span class="n">hidden_states_a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_to_out</span><span class="p">(</span><span class="n">hidden_states_a</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">only_out_a</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hidden_states_a</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hidden_states_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_to_out</span><span class="p">(</span><span class="n">hidden_states_b</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">hidden_states_a</span><span class="p">,</span> <span class="n">hidden_states_b</span></div>
</div>




<div class="viewcode-block" id="JointTransformerBlock">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.JointTransformerBlock">[docs]</a>
<span class="k">class</span> <span class="nc">JointTransformerBlock</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1_a</span> <span class="o">=</span> <span class="n">AdaLayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1_b</span> <span class="o">=</span> <span class="n">AdaLayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">JointAttention</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_attention_heads</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">norm2_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="o">*</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(</span><span class="n">approximate</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">norm2_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="o">*</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(</span><span class="n">approximate</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="p">)</span>


<div class="viewcode-block" id="JointTransformerBlock.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.JointTransformerBlock.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states_a</span><span class="p">,</span> <span class="n">hidden_states_b</span><span class="p">,</span> <span class="n">temb</span><span class="p">):</span>
        <span class="n">norm_hidden_states_a</span><span class="p">,</span> <span class="n">gate_msa_a</span><span class="p">,</span> <span class="n">shift_mlp_a</span><span class="p">,</span> <span class="n">scale_mlp_a</span><span class="p">,</span> <span class="n">gate_mlp_a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1_a</span><span class="p">(</span><span class="n">hidden_states_a</span><span class="p">,</span> <span class="n">emb</span><span class="o">=</span><span class="n">temb</span><span class="p">)</span>
        <span class="n">norm_hidden_states_b</span><span class="p">,</span> <span class="n">gate_msa_b</span><span class="p">,</span> <span class="n">shift_mlp_b</span><span class="p">,</span> <span class="n">scale_mlp_b</span><span class="p">,</span> <span class="n">gate_mlp_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1_b</span><span class="p">(</span><span class="n">hidden_states_b</span><span class="p">,</span> <span class="n">emb</span><span class="o">=</span><span class="n">temb</span><span class="p">)</span>

        <span class="c1"># Attention</span>
        <span class="n">attn_output_a</span><span class="p">,</span> <span class="n">attn_output_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">norm_hidden_states_a</span><span class="p">,</span> <span class="n">norm_hidden_states_b</span><span class="p">)</span>

        <span class="c1"># Part A</span>
        <span class="n">hidden_states_a</span> <span class="o">=</span> <span class="n">hidden_states_a</span> <span class="o">+</span> <span class="n">gate_msa_a</span> <span class="o">*</span> <span class="n">attn_output_a</span>
        <span class="n">norm_hidden_states_a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2_a</span><span class="p">(</span><span class="n">hidden_states_a</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale_mlp_a</span><span class="p">)</span> <span class="o">+</span> <span class="n">shift_mlp_a</span>
        <span class="n">hidden_states_a</span> <span class="o">=</span> <span class="n">hidden_states_a</span> <span class="o">+</span> <span class="n">gate_mlp_a</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_a</span><span class="p">(</span><span class="n">norm_hidden_states_a</span><span class="p">)</span>

        <span class="c1"># Part B</span>
        <span class="n">hidden_states_b</span> <span class="o">=</span> <span class="n">hidden_states_b</span> <span class="o">+</span> <span class="n">gate_msa_b</span> <span class="o">*</span> <span class="n">attn_output_b</span>
        <span class="n">norm_hidden_states_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2_b</span><span class="p">(</span><span class="n">hidden_states_b</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale_mlp_b</span><span class="p">)</span> <span class="o">+</span> <span class="n">shift_mlp_b</span>
        <span class="n">hidden_states_b</span> <span class="o">=</span> <span class="n">hidden_states_b</span> <span class="o">+</span> <span class="n">gate_mlp_b</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_b</span><span class="p">(</span><span class="n">norm_hidden_states_b</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states_a</span><span class="p">,</span> <span class="n">hidden_states_b</span></div>
</div>




<div class="viewcode-block" id="JointTransformerFinalBlock">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.JointTransformerFinalBlock">[docs]</a>
<span class="k">class</span> <span class="nc">JointTransformerFinalBlock</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1_a</span> <span class="o">=</span> <span class="n">AdaLayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1_b</span> <span class="o">=</span> <span class="n">AdaLayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">single</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">JointAttention</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">only_out_a</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">norm2_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="o">*</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(</span><span class="n">approximate</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="p">)</span>


<div class="viewcode-block" id="JointTransformerFinalBlock.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.JointTransformerFinalBlock.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states_a</span><span class="p">,</span> <span class="n">hidden_states_b</span><span class="p">,</span> <span class="n">temb</span><span class="p">):</span>
        <span class="n">norm_hidden_states_a</span><span class="p">,</span> <span class="n">gate_msa_a</span><span class="p">,</span> <span class="n">shift_mlp_a</span><span class="p">,</span> <span class="n">scale_mlp_a</span><span class="p">,</span> <span class="n">gate_mlp_a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1_a</span><span class="p">(</span><span class="n">hidden_states_a</span><span class="p">,</span> <span class="n">emb</span><span class="o">=</span><span class="n">temb</span><span class="p">)</span>
        <span class="n">norm_hidden_states_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1_b</span><span class="p">(</span><span class="n">hidden_states_b</span><span class="p">,</span> <span class="n">emb</span><span class="o">=</span><span class="n">temb</span><span class="p">)</span>

        <span class="c1"># Attention</span>
        <span class="n">attn_output_a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">norm_hidden_states_a</span><span class="p">,</span> <span class="n">norm_hidden_states_b</span><span class="p">)</span>

        <span class="c1"># Part A</span>
        <span class="n">hidden_states_a</span> <span class="o">=</span> <span class="n">hidden_states_a</span> <span class="o">+</span> <span class="n">gate_msa_a</span> <span class="o">*</span> <span class="n">attn_output_a</span>
        <span class="n">norm_hidden_states_a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2_a</span><span class="p">(</span><span class="n">hidden_states_a</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale_mlp_a</span><span class="p">)</span> <span class="o">+</span> <span class="n">shift_mlp_a</span>
        <span class="n">hidden_states_a</span> <span class="o">=</span> <span class="n">hidden_states_a</span> <span class="o">+</span> <span class="n">gate_mlp_a</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_a</span><span class="p">(</span><span class="n">norm_hidden_states_a</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states_a</span><span class="p">,</span> <span class="n">hidden_states_b</span></div>
</div>




<div class="viewcode-block" id="SD3DiT">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.SD3DiT">[docs]</a>
<span class="k">class</span> <span class="nc">SD3DiT</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedder</span> <span class="o">=</span> <span class="n">PatchEmbed</span><span class="p">(</span><span class="n">patch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">1536</span><span class="p">,</span> <span class="n">pos_embed_max_size</span><span class="o">=</span><span class="mi">192</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_embedder</span> <span class="o">=</span> <span class="n">TimestepEmbeddings</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1536</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_text_embedder</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">1536</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1536</span><span class="p">,</span> <span class="mi">1536</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_embedder</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="mi">1536</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">JointTransformerBlock</span><span class="p">(</span><span class="mi">1536</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">23</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">JointTransformerFinalBlock</span><span class="p">(</span><span class="mi">1536</span><span class="p">,</span> <span class="mi">24</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_out</span> <span class="o">=</span> <span class="n">AdaLayerNorm</span><span class="p">(</span><span class="mi">1536</span><span class="p">,</span> <span class="n">single</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1536</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>

<div class="viewcode-block" id="SD3DiT.tiled_forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.SD3DiT.tiled_forward">[docs]</a>
    <span class="k">def</span> <span class="nf">tiled_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">prompt_emb</span><span class="p">,</span> <span class="n">pooled_prompt_emb</span><span class="p">,</span> <span class="n">tile_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">tile_stride</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="c1"># Due to the global positional embedding, we cannot implement layer-wise tiled forward.</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">TileWorker</span><span class="p">()</span><span class="o">.</span><span class="n">tiled_forward</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">prompt_emb</span><span class="p">,</span> <span class="n">pooled_prompt_emb</span><span class="p">),</span>
            <span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">tile_size</span><span class="p">,</span>
            <span class="n">tile_stride</span><span class="p">,</span>
            <span class="n">tile_device</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">tile_dtype</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span></div>


<div class="viewcode-block" id="SD3DiT.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.SD3DiT.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">prompt_emb</span><span class="p">,</span> <span class="n">pooled_prompt_emb</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">tile_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">tile_stride</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">use_gradient_checkpointing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tiled</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tiled_forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">prompt_emb</span><span class="p">,</span> <span class="n">pooled_prompt_emb</span><span class="p">,</span> <span class="n">tile_size</span><span class="p">,</span> <span class="n">tile_stride</span><span class="p">)</span>
        <span class="n">conditioning</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_embedder</span><span class="p">(</span><span class="n">timestep</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooled_text_embedder</span><span class="p">(</span><span class="n">pooled_prompt_emb</span><span class="p">)</span>
        <span class="n">prompt_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_embedder</span><span class="p">(</span><span class="n">prompt_emb</span><span class="p">)</span>

        <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedder</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">create_custom_forward</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="k">def</span> <span class="nf">custom_forward</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">custom_forward</span>
        
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="n">use_gradient_checkpointing</span><span class="p">:</span>
                <span class="n">hidden_states</span><span class="p">,</span> <span class="n">prompt_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span>
                    <span class="n">create_custom_forward</span><span class="p">(</span><span class="n">block</span><span class="p">),</span>
                    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">prompt_emb</span><span class="p">,</span> <span class="n">conditioning</span><span class="p">,</span>
                    <span class="n">use_reentrant</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">hidden_states</span><span class="p">,</span> <span class="n">prompt_emb</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">prompt_emb</span><span class="p">,</span> <span class="n">conditioning</span><span class="p">)</span>
        
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_out</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">conditioning</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_out</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="s2">&quot;B (H W) (P Q C) -&gt; B C (H P) (W Q)&quot;</span><span class="p">,</span> <span class="n">P</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">Q</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="n">height</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">width</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span></div>

        
<div class="viewcode-block" id="SD3DiT.state_dict_converter">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.SD3DiT.state_dict_converter">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">state_dict_converter</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">SD3DiTStateDictConverter</span><span class="p">()</span></div>
</div>




<div class="viewcode-block" id="SD3DiTStateDictConverter">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.SD3DiTStateDictConverter">[docs]</a>
<span class="k">class</span> <span class="nc">SD3DiTStateDictConverter</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

<div class="viewcode-block" id="SD3DiTStateDictConverter.from_diffusers">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.SD3DiTStateDictConverter.from_diffusers">[docs]</a>
    <span class="k">def</span> <span class="nf">from_diffusers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
        <span class="n">rename_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;context_embedder&quot;</span><span class="p">:</span> <span class="s2">&quot;context_embedder&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pos_embed.pos_embed&quot;</span><span class="p">:</span> <span class="s2">&quot;pos_embedder.pos_embed&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pos_embed.proj&quot;</span><span class="p">:</span> <span class="s2">&quot;pos_embedder.proj&quot;</span><span class="p">,</span>
            <span class="s2">&quot;time_text_embed.timestep_embedder.linear_1&quot;</span><span class="p">:</span> <span class="s2">&quot;time_embedder.timestep_embedder.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;time_text_embed.timestep_embedder.linear_2&quot;</span><span class="p">:</span> <span class="s2">&quot;time_embedder.timestep_embedder.2&quot;</span><span class="p">,</span>
            <span class="s2">&quot;time_text_embed.text_embedder.linear_1&quot;</span><span class="p">:</span> <span class="s2">&quot;pooled_text_embedder.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;time_text_embed.text_embedder.linear_2&quot;</span><span class="p">:</span> <span class="s2">&quot;pooled_text_embedder.2&quot;</span><span class="p">,</span>
            <span class="s2">&quot;norm_out.linear&quot;</span><span class="p">:</span> <span class="s2">&quot;norm_out.linear&quot;</span><span class="p">,</span>
            <span class="s2">&quot;proj_out&quot;</span><span class="p">:</span> <span class="s2">&quot;proj_out&quot;</span><span class="p">,</span>

            <span class="s2">&quot;norm1.linear&quot;</span><span class="p">:</span> <span class="s2">&quot;norm1_a.linear&quot;</span><span class="p">,</span>
            <span class="s2">&quot;norm1_context.linear&quot;</span><span class="p">:</span> <span class="s2">&quot;norm1_b.linear&quot;</span><span class="p">,</span>
            <span class="s2">&quot;attn.to_q&quot;</span><span class="p">:</span> <span class="s2">&quot;attn.a_to_q&quot;</span><span class="p">,</span>
            <span class="s2">&quot;attn.to_k&quot;</span><span class="p">:</span> <span class="s2">&quot;attn.a_to_k&quot;</span><span class="p">,</span>
            <span class="s2">&quot;attn.to_v&quot;</span><span class="p">:</span> <span class="s2">&quot;attn.a_to_v&quot;</span><span class="p">,</span>
            <span class="s2">&quot;attn.to_out.0&quot;</span><span class="p">:</span> <span class="s2">&quot;attn.a_to_out&quot;</span><span class="p">,</span>
            <span class="s2">&quot;attn.add_q_proj&quot;</span><span class="p">:</span> <span class="s2">&quot;attn.b_to_q&quot;</span><span class="p">,</span>
            <span class="s2">&quot;attn.add_k_proj&quot;</span><span class="p">:</span> <span class="s2">&quot;attn.b_to_k&quot;</span><span class="p">,</span>
            <span class="s2">&quot;attn.add_v_proj&quot;</span><span class="p">:</span> <span class="s2">&quot;attn.b_to_v&quot;</span><span class="p">,</span>
            <span class="s2">&quot;attn.to_add_out&quot;</span><span class="p">:</span> <span class="s2">&quot;attn.b_to_out&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ff.net.0.proj&quot;</span><span class="p">:</span> <span class="s2">&quot;ff_a.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ff.net.2&quot;</span><span class="p">:</span> <span class="s2">&quot;ff_a.2&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ff_context.net.0.proj&quot;</span><span class="p">:</span> <span class="s2">&quot;ff_b.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ff_context.net.2&quot;</span><span class="p">:</span> <span class="s2">&quot;ff_b.2&quot;</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">state_dict_</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">rename_dict</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;pos_embed.pos_embed&quot;</span><span class="p">:</span>
                    <span class="n">param</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">1536</span><span class="p">))</span>
                <span class="n">state_dict_</span><span class="p">[</span><span class="n">rename_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]]</span> <span class="o">=</span> <span class="n">param</span>
            <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.weight&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.bias&quot;</span><span class="p">):</span>
                <span class="n">suffix</span> <span class="o">=</span> <span class="s2">&quot;.weight&quot;</span> <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.weight&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;.bias&quot;</span>
                <span class="n">prefix</span> <span class="o">=</span> <span class="n">name</span><span class="p">[:</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">suffix</span><span class="p">)]</span>
                <span class="k">if</span> <span class="n">prefix</span> <span class="ow">in</span> <span class="n">rename_dict</span><span class="p">:</span>
                    <span class="n">state_dict_</span><span class="p">[</span><span class="n">rename_dict</span><span class="p">[</span><span class="n">prefix</span><span class="p">]</span> <span class="o">+</span> <span class="n">suffix</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
                <span class="k">elif</span> <span class="n">prefix</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;transformer_blocks.&quot;</span><span class="p">):</span>
                    <span class="n">names</span> <span class="o">=</span> <span class="n">prefix</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
                    <span class="n">names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;blocks&quot;</span>
                    <span class="n">middle</span> <span class="o">=</span> <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
                    <span class="k">if</span> <span class="n">middle</span> <span class="ow">in</span> <span class="n">rename_dict</span><span class="p">:</span>
                        <span class="n">name_</span> <span class="o">=</span> <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">names</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">rename_dict</span><span class="p">[</span><span class="n">middle</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[</span><span class="n">suffix</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
                        <span class="n">state_dict_</span><span class="p">[</span><span class="n">name_</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
        <span class="k">return</span> <span class="n">state_dict_</span></div>

    
<div class="viewcode-block" id="SD3DiTStateDictConverter.from_civitai">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.sd3_dit.SD3DiTStateDictConverter.from_civitai">[docs]</a>
    <span class="k">def</span> <span class="nf">from_civitai</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
        <span class="n">rename_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;model.diffusion_model.context_embedder.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;context_embedder.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.context_embedder.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;context_embedder.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.final_layer.linear.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;proj_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.final_layer.linear.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;proj_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.0.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.0.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.0.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.0.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.0.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.0.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.0.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.0.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.0.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.0.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.0.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.0.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.0.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.0.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.1.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.1.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.1.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.1.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.1.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.1.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.1.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.1.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.1.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.1.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.1.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.1.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.1.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.1.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.10.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.10.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.10.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.10.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.10.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.10.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.10.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.10.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.10.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.10.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.10.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.10.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.10.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.10.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.11.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.11.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.11.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.11.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.11.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.11.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.11.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.11.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.11.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.11.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.11.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.11.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.11.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.11.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.12.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.12.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.12.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.12.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.12.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.12.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.12.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.12.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.12.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.12.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.12.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.12.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.12.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.12.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.13.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.13.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.13.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.13.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.13.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.13.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.13.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.13.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.13.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.13.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.13.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.13.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.13.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.13.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.14.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.14.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.14.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.14.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.14.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.14.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.14.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.14.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.14.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.14.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.14.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.14.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.14.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.14.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.15.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.15.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.15.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.15.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.15.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.15.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.15.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.15.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.15.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.15.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.15.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.15.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.15.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.15.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.16.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.16.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.16.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.16.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.16.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.16.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.16.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.16.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.16.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.16.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.16.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.16.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.16.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.16.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.17.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.17.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.17.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.17.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.17.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.17.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.17.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.17.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.17.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.17.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.17.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.17.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.17.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.17.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.18.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.18.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.18.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.18.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.18.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.18.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.18.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.18.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.18.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.18.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.18.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.18.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.18.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.18.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.19.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.19.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.19.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.19.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.19.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.19.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.19.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.19.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.19.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.19.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.19.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.19.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.19.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.19.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.2.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.2.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.2.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.2.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.2.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.2.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.2.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.2.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.2.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.2.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.2.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.2.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.2.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.2.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.20.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.20.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.20.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.20.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.20.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.20.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.20.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.20.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.20.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.20.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.20.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.20.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.20.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.20.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.21.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.21.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.21.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.21.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.21.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.21.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.21.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.21.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.21.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.21.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.21.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.21.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.21.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.21.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.22.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.22.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.22.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.22.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.22.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.22.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.22.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.22.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.22.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.22.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.22.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.22.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.22.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.22.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.23.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.23.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.23.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.23.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.23.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.23.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.23.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.23.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.23.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.23.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.23.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.23.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.23.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.23.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.23.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.23.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.23.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.23.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.23.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.23.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.3.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.3.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.3.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.3.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.3.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.3.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.3.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.3.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.3.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.3.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.3.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.3.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.3.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.3.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.4.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.4.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.4.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.4.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.4.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.4.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.4.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.4.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.4.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.4.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.4.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.4.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.4.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.4.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.5.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.5.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.5.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.5.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.5.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.5.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.5.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.5.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.5.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.5.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.5.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.5.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.5.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.5.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.6.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.6.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.6.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.6.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.6.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.6.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.6.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.6.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.6.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.6.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.6.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.6.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.6.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.6.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.7.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.7.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.7.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.7.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.7.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.7.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.7.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.7.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.7.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.7.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.7.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.7.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.7.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.7.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.8.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.8.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.8.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.8.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.8.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.8.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.8.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.8.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.8.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.8.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.8.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.8.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.8.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.8.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.context_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.attn.b_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.context_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.attn.b_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.context_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.9.attn.b_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.9.attn.b_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.9.attn.b_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.context_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.9.attn.b_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.9.attn.b_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.9.attn.b_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.context_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.ff_b.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.context_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.ff_b.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.context_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.ff_b.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.context_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.ff_b.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.x_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.norm1_a.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.x_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.norm1_a.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.x_block.attn.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.attn.a_to_out.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.x_block.attn.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.attn.a_to_out.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.x_block.attn.qkv.bias&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.9.attn.a_to_q.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.9.attn.a_to_k.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.9.attn.a_to_v.bias&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.x_block.attn.qkv.weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;blocks.9.attn.a_to_q.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.9.attn.a_to_k.weight&#39;</span><span class="p">,</span> <span class="s1">&#39;blocks.9.attn.a_to_v.weight&#39;</span><span class="p">],</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.x_block.mlp.fc1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.ff_a.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.x_block.mlp.fc1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.ff_a.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.x_block.mlp.fc2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.ff_a.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.9.x_block.mlp.fc2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.9.ff_a.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.pos_embed&quot;</span><span class="p">:</span> <span class="s2">&quot;pos_embedder.pos_embed&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.t_embedder.mlp.0.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;time_embedder.timestep_embedder.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.t_embedder.mlp.0.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;time_embedder.timestep_embedder.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.t_embedder.mlp.2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;time_embedder.timestep_embedder.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.t_embedder.mlp.2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;time_embedder.timestep_embedder.2.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.x_embedder.proj.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;pos_embedder.proj.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.x_embedder.proj.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;pos_embedder.proj.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.y_embedder.mlp.0.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;pooled_text_embedder.0.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.y_embedder.mlp.0.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;pooled_text_embedder.0.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.y_embedder.mlp.2.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;pooled_text_embedder.2.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.y_embedder.mlp.2.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;pooled_text_embedder.2.weight&quot;</span><span class="p">,</span>
            
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.context_block.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.23.norm1_b.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.joint_blocks.23.context_block.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;blocks.23.norm1_b.linear.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.final_layer.adaLN_modulation.1.weight&quot;</span><span class="p">:</span> <span class="s2">&quot;norm_out.linear.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;model.diffusion_model.final_layer.adaLN_modulation.1.bias&quot;</span><span class="p">:</span> <span class="s2">&quot;norm_out.linear.bias&quot;</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">state_dict_</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">rename_dict</span><span class="p">:</span>
                <span class="n">param</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;model.diffusion_model.joint_blocks.23.context_block.adaLN_modulation.1.&quot;</span><span class="p">):</span>
                    <span class="n">param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">param</span><span class="p">[</span><span class="mi">1536</span><span class="p">:],</span> <span class="n">param</span><span class="p">[:</span><span class="mi">1536</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;model.diffusion_model.final_layer.adaLN_modulation.1.&quot;</span><span class="p">):</span>
                    <span class="n">param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">param</span><span class="p">[</span><span class="mi">1536</span><span class="p">:],</span> <span class="n">param</span><span class="p">[:</span><span class="mi">1536</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;model.diffusion_model.pos_embed&quot;</span><span class="p">:</span>
                    <span class="n">param</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">1536</span><span class="p">))</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">rename_dict</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
                    <span class="n">state_dict_</span><span class="p">[</span><span class="n">rename_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]]</span> <span class="o">=</span> <span class="n">param</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">name_</span> <span class="o">=</span> <span class="n">rename_dict</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.a_to_q.&quot;</span><span class="p">,</span> <span class="s2">&quot;.a_to_qkv.&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.b_to_q.&quot;</span><span class="p">,</span> <span class="s2">&quot;.b_to_qkv.&quot;</span><span class="p">)</span>
                    <span class="n">state_dict_</span><span class="p">[</span><span class="n">name_</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
        <span class="k">return</span> <span class="n">state_dict_</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, ModelScope.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>