<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>diffsynth.models.kolors_text_encoder &mdash; DiffSynth-Studio 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=01f34227"></script>
        <script src="../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            DiffSynth-Studio
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"></div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">DiffSynth-Studio</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">diffsynth.models.kolors_text_encoder</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for diffsynth.models.kolors_text_encoder</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This model is copied from https://github.com/Kwai-Kolors/Kolors/tree/master/kolors/models.</span>
<span class="sd">We didn&#39;t modify this model.</span>
<span class="sd">The tensor operation is performed in the prompter.</span>
<span class="sd">&quot;&quot;&quot;</span>


<span class="sd">&quot;&quot;&quot; PyTorch ChatGLM model. &quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.utils.checkpoint</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span><span class="p">,</span> <span class="n">LayerNorm</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span><span class="p">,</span> <span class="n">LayerNorm</span><span class="p">,</span> <span class="n">MSELoss</span><span class="p">,</span> <span class="n">BCEWithLogitsLoss</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils</span> <span class="kn">import</span> <span class="n">skip_init</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>

<span class="kn">from</span> <span class="nn">transformers.modeling_outputs</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">BaseModelOutputWithPast</span><span class="p">,</span>
    <span class="n">CausalLMOutputWithPast</span><span class="p">,</span>
    <span class="n">SequenceClassifierOutputWithPast</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">transformers.modeling_utils</span> <span class="kn">import</span> <span class="n">PreTrainedModel</span>
<span class="kn">from</span> <span class="nn">transformers.utils</span> <span class="kn">import</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">transformers.generation.logits_process</span> <span class="kn">import</span> <span class="n">LogitsProcessor</span>
<span class="kn">from</span> <span class="nn">transformers.generation.utils</span> <span class="kn">import</span> <span class="n">LogitsProcessorList</span><span class="p">,</span> <span class="n">StoppingCriteriaList</span><span class="p">,</span> <span class="n">GenerationConfig</span><span class="p">,</span> <span class="n">ModelOutput</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">PretrainedConfig</span>



<div class="viewcode-block" id="ChatGLMConfig">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMConfig">[docs]</a>
<span class="k">class</span> <span class="nc">ChatGLMConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;chatglm&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="mi">28</span><span class="p">,</span>
        <span class="n">padded_vocab_size</span><span class="o">=</span><span class="mi">65024</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
        <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">13696</span><span class="p">,</span>
        <span class="n">kv_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">seq_length</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
        <span class="n">hidden_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">classifier_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">layernorm_epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">rmsnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">apply_residual_connection_post_layernorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">post_layer_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">add_bias_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">add_qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">bias_dropout_fusion</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">multi_query_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">multi_query_group_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">apply_query_key_layer_scaling</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">attention_softmax_in_fp32</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">fp32_residual_connection</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">quantization_bit</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">pre_seq_len</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">prefix_projection</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">padded_vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padded_vocab_size</span> <span class="o">=</span> <span class="n">padded_vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_hidden_size</span> <span class="o">=</span> <span class="n">ffn_hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kv_channels</span> <span class="o">=</span> <span class="n">kv_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dropout</span> <span class="o">=</span> <span class="n">hidden_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier_dropout</span> <span class="o">=</span> <span class="n">classifier_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_epsilon</span> <span class="o">=</span> <span class="n">layernorm_epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rmsnorm</span> <span class="o">=</span> <span class="n">rmsnorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply_residual_connection_post_layernorm</span> <span class="o">=</span> <span class="n">apply_residual_connection_post_layernorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_layer_norm</span> <span class="o">=</span> <span class="n">post_layer_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_bias_linear</span> <span class="o">=</span> <span class="n">add_bias_linear</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_qkv_bias</span> <span class="o">=</span> <span class="n">add_qkv_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_dropout_fusion</span> <span class="o">=</span> <span class="n">bias_dropout_fusion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_attention</span> <span class="o">=</span> <span class="n">multi_query_attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_group_num</span> <span class="o">=</span> <span class="n">multi_query_group_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply_query_key_layer_scaling</span> <span class="o">=</span> <span class="n">apply_query_key_layer_scaling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_softmax_in_fp32</span> <span class="o">=</span> <span class="n">attention_softmax_in_fp32</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fp32_residual_connection</span> <span class="o">=</span> <span class="n">fp32_residual_connection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="n">quantization_bit</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_seq_len</span> <span class="o">=</span> <span class="n">pre_seq_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefix_projection</span> <span class="o">=</span> <span class="n">prefix_projection</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>




<span class="c1"># flags required to enable jit fusion kernels</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">!=</span> <span class="s1">&#39;darwin&#39;</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_set_profiling_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_set_profiling_executor</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_override_can_fuse_on_cpu</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_override_can_fuse_on_gpu</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">_CHECKPOINT_FOR_DOC</span> <span class="o">=</span> <span class="s2">&quot;THUDM/ChatGLM&quot;</span>
<span class="n">_CONFIG_FOR_DOC</span> <span class="o">=</span> <span class="s2">&quot;ChatGLM6BConfig&quot;</span>

<span class="n">CHATGLM_6B_PRETRAINED_MODEL_ARCHIVE_LIST</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;THUDM/chatglm3-6b-base&quot;</span><span class="p">,</span>
    <span class="c1"># See all ChatGLM models at https://huggingface.co/models?filter=chatglm</span>
<span class="p">]</span>


<div class="viewcode-block" id="default_init">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.default_init">[docs]</a>
<span class="k">def</span> <span class="nf">default_init</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>



<div class="viewcode-block" id="InvalidScoreLogitsProcessor">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.InvalidScoreLogitsProcessor">[docs]</a>
<span class="k">class</span> <span class="nc">InvalidScoreLogitsProcessor</span><span class="p">(</span><span class="n">LogitsProcessor</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="n">scores</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5e4</span>
        <span class="k">return</span> <span class="n">scores</span></div>



<div class="viewcode-block" id="PrefixEncoder">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.PrefixEncoder">[docs]</a>
<span class="k">class</span> <span class="nc">PrefixEncoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The torch.nn model to encode the prefix</span>
<span class="sd">    Input shape: (batch-size, prefix-length)</span>
<span class="sd">    Output shape: (batch-size, prefix-length, 2*layers*hidden)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ChatGLMConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefix_projection</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">prefix_projection</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_projection</span><span class="p">:</span>
            <span class="c1"># Use a two-layer MLP to encode the prefix</span>
            <span class="n">kv_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">kv_channels</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">multi_query_group_num</span> <span class="o">*</span> <span class="mi">2</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">pre_seq_len</span><span class="p">,</span> <span class="n">kv_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trans</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">kv_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">kv_size</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">pre_seq_len</span><span class="p">,</span>
                                                <span class="n">config</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">kv_channels</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">multi_query_group_num</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>

<div class="viewcode-block" id="PrefixEncoder.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.PrefixEncoder.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_projection</span><span class="p">:</span>
            <span class="n">prefix_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trans</span><span class="p">(</span><span class="n">prefix_tokens</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">past_key_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">past_key_values</span></div>
</div>



<div class="viewcode-block" id="split_tensor_along_last_dim">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.split_tensor_along_last_dim">[docs]</a>
<span class="k">def</span> <span class="nf">split_tensor_along_last_dim</span><span class="p">(</span>
        <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">contiguous_split_chunks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Split a tensor along its last dimension.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor: input tensor.</span>
<span class="sd">        num_partitions: number of partitions to split the tensor</span>
<span class="sd">        contiguous_split_chunks: If True, make each chunk contiguous</span>
<span class="sd">                                 in memory.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of Tensors</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the size and dimension.</span>
    <span class="n">last_dim</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">last_dim_size</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="n">last_dim</span><span class="p">]</span> <span class="o">//</span> <span class="n">num_partitions</span>
    <span class="c1"># Split.</span>
    <span class="n">tensor_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">last_dim_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">last_dim</span><span class="p">)</span>
    <span class="c1"># Note: torch.split does not create contiguous tensors by default.</span>
    <span class="k">if</span> <span class="n">contiguous_split_chunks</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">tensor_list</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tensor_list</span></div>



<div class="viewcode-block" id="RotaryEmbedding">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.RotaryEmbedding">[docs]</a>
<span class="k">class</span> <span class="nc">RotaryEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">original_impl</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">/</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;inv_freq&quot;</span><span class="p">,</span> <span class="n">inv_freq</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">original_impl</span> <span class="o">=</span> <span class="n">original_impl</span>

<div class="viewcode-block" id="RotaryEmbedding.forward_impl">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.RotaryEmbedding.forward_impl">[docs]</a>
    <span class="k">def</span> <span class="nf">forward_impl</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_elem</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">base</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Enhanced Transformer with Rotary Position Embedding.</span>

<span class="sd">        Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/</span>
<span class="sd">        transformers/rope/__init__.py. MIT License:</span>
<span class="sd">        https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># $\Theta = {\theta_i = 10000^{\frac{2(i-1)}{d}}, i \in [1, 2, ..., \frac{d}{2}]}$</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">base</span> <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_elem</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_elem</span><span class="p">))</span>

        <span class="c1"># Create position indexes `[0, 1, ..., seq_len - 1]`</span>
        <span class="n">seq_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Calculate the product of position index and $\theta_i$</span>
        <span class="n">idx_theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">seq_idx</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="n">cache</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">idx_theta</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">idx_theta</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># this is to mimic the behaviour of complex32, else we will get different results</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">):</span>
            <span class="n">cache</span> <span class="o">=</span> <span class="n">cache</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">()</span> <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="k">else</span> <span class="n">cache</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">cache</span></div>


<div class="viewcode-block" id="RotaryEmbedding.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.RotaryEmbedding.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_impl</span><span class="p">(</span>
            <span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span></div>
</div>



<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">rope_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># x: [sq, b, np, hn]</span>
    <span class="n">sq</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">np</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">rot_dim</span> <span class="o">=</span> <span class="n">rope_cache</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">x_pass</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">rot_dim</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">rot_dim</span><span class="p">:]</span>
    <span class="c1"># truncate to support variable sizes</span>
    <span class="n">rope_cache</span> <span class="o">=</span> <span class="n">rope_cache</span><span class="p">[:</span><span class="n">sq</span><span class="p">]</span>
    <span class="n">xshaped</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sq</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="p">,</span> <span class="n">rot_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">rope_cache</span> <span class="o">=</span> <span class="n">rope_cache</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">sq</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xshaped</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">x_out2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">xshaped</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">rope_cache</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">xshaped</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">rope_cache</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">xshaped</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">rope_cache</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">xshaped</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">rope_cache</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">],</span>
        <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">x_out2</span> <span class="o">=</span> <span class="n">x_out2</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x_out2</span><span class="p">,</span> <span class="n">x_pass</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<div class="viewcode-block" id="RMSNorm">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.RMSNorm">[docs]</a>
<span class="k">class</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>

<div class="viewcode-block" id="RMSNorm.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.RMSNorm.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">variance</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="CoreAttention">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.CoreAttention">[docs]</a>
<span class="k">class</span> <span class="nc">CoreAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ChatGLMConfig</span><span class="p">,</span> <span class="n">layer_number</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CoreAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_query_key_layer_scaling</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">apply_query_key_layer_scaling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_softmax_in_fp32</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">attention_softmax_in_fp32</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_query_key_layer_scaling</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention_softmax_in_fp32</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_number</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">layer_number</span><span class="p">)</span>

        <span class="n">projection_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">kv_channels</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>

        <span class="c1"># Per attention head and per partition values.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_partition</span> <span class="o">=</span> <span class="n">projection_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span> <span class="o">=</span> <span class="n">projection_size</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads_per_partition</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>

        <span class="n">coeff</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_factor</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_query_key_layer_scaling</span><span class="p">:</span>
            <span class="n">coeff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_number</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm_factor</span> <span class="o">*=</span> <span class="n">coeff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coeff</span> <span class="o">=</span> <span class="n">coeff</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">)</span>

<div class="viewcode-block" id="CoreAttention.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.CoreAttention.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
        <span class="n">pytorch_major_version</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">pytorch_major_version</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">]]</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">query_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">key_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
                <span class="n">context_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">,</span>
                                                                                 <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">attention_mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">attention_mask</span>
                <span class="n">context_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">,</span>
                                                                                 <span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">context_layer</span> <span class="o">=</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
            <span class="n">new_context_layer_shape</span> <span class="o">=</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_partition</span><span class="p">,)</span>
            <span class="n">context_layer</span> <span class="o">=</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">new_context_layer_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Raw attention scores</span>

            <span class="c1"># [b, np, sq, sk]</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">query_layer</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">query_layer</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">query_layer</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">key_layer</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

            <span class="c1"># [sq, b, np, hn] -&gt; [sq, b * np, hn]</span>
            <span class="n">query_layer</span> <span class="o">=</span> <span class="n">query_layer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">output_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># [sk, b, np, hn] -&gt; [sk, b * np, hn]</span>
            <span class="n">key_layer</span> <span class="o">=</span> <span class="n">key_layer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">output_size</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># preallocting input tensor: [b * np, sq, sk]</span>
            <span class="n">matmul_input_buffer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_layer</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">query_layer</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>

            <span class="c1"># Raw attention scores. [b * np, sq, sk]</span>
            <span class="n">matmul_result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">baddbmm</span><span class="p">(</span>
                <span class="n">matmul_input_buffer</span><span class="p">,</span>
                <span class="n">query_layer</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>  <span class="c1"># [b * np, sq, hn]</span>
                <span class="n">key_layer</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>  <span class="c1"># [b * np, hn, sk]</span>
                <span class="n">beta</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_factor</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="c1"># change view to [b, np, sq, sk]</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">matmul_result</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">output_size</span><span class="p">)</span>

            <span class="c1"># ===========================</span>
            <span class="c1"># Attention probs and dropout</span>
            <span class="c1"># ===========================</span>

            <span class="c1"># attention scores and attention mask [b, np, sq, sk]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_softmax_in_fp32</span><span class="p">:</span>
                <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">coeff</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">coeff</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
                                            <span class="n">device</span><span class="o">=</span><span class="n">attention_scores</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
                <span class="n">attention_mask</span><span class="o">.</span><span class="n">tril_</span><span class="p">()</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">attention_mask</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attention_probs</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">value_layer</span><span class="p">)</span>

            <span class="c1"># This is actually dropping out entire tokens to attend to, which might</span>
            <span class="c1"># seem a bit unusual, but is taken from the original Transformer paper.</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">)</span>
            <span class="c1"># =========================</span>
            <span class="c1"># Context layer. [sq, b, hp]</span>
            <span class="c1"># =========================</span>

            <span class="c1"># value_layer -&gt; context layer.</span>
            <span class="c1"># [sk, b, np, hn] --&gt; [b, np, sq, hn]</span>

            <span class="c1"># context layer shape: [b, np, sq, hn]</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">value_layer</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">value_layer</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">query_layer</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">value_layer</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
            <span class="c1"># change view [sk, b * np, hn]</span>
            <span class="n">value_layer</span> <span class="o">=</span> <span class="n">value_layer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">value_layer</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># change view [b * np, sq, sk]</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attention_probs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># matmul: [b * np, sq, hn]</span>
            <span class="n">context_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value_layer</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="c1"># change view [b, np, sq, hn]</span>
            <span class="n">context_layer</span> <span class="o">=</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">output_size</span><span class="p">)</span>
            <span class="c1"># [b, np, sq, hn] --&gt; [sq, b, np, hn]</span>
            <span class="n">context_layer</span> <span class="o">=</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="c1"># [sq, b, np, hn] --&gt; [sq, b, hp]</span>
            <span class="n">new_context_layer_shape</span> <span class="o">=</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_partition</span><span class="p">,)</span>
            <span class="n">context_layer</span> <span class="o">=</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">new_context_layer_shape</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">context_layer</span></div>
</div>



<div class="viewcode-block" id="SelfAttention">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.SelfAttention">[docs]</a>
<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Parallel self-attention layer abstract class.</span>

<span class="sd">    Self-attention layer takes input with size [s, b, h]</span>
<span class="sd">    and returns output of the same size.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ChatGLMConfig</span><span class="p">,</span> <span class="n">layer_number</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_number</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">layer_number</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">projection_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">kv_channels</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>

        <span class="c1"># Per attention head and per partition values.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_size</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads_per_partition</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_attention</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">multi_query_attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qkv_hidden_size</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_size</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_attention</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_multi_query_groups_per_partition</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">multi_query_group_num</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qkv_hidden_size</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">projection_size</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">multi_query_group_num</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_key_value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv_hidden_size</span><span class="p">,</span>
                                         <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">add_bias_linear</span> <span class="ow">or</span> <span class="n">config</span><span class="o">.</span><span class="n">add_qkv_bias</span><span class="p">,</span>
                                         <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">_config_to_kwargs</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
                                         <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">core_attention</span> <span class="o">=</span> <span class="n">CoreAttention</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_number</span><span class="p">)</span>

        <span class="c1"># Output.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projection_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">add_bias_linear</span><span class="p">,</span>
                               <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">_config_to_kwargs</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
                               <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_allocate_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_max_sequence_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_attention</span><span class="p">:</span>
            <span class="n">num_attention_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_multi_query_groups_per_partition</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">num_attention_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads_per_partition</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="n">inference_max_sequence_len</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="p">,</span>
            <span class="n">num_attention_heads</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="SelfAttention.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.SelfAttention.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">rotary_pos_emb</span><span class="p">,</span> <span class="n">kv_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">):</span>
        <span class="c1"># hidden_states: [sq, b, h]</span>

        <span class="c1"># =================================================</span>
        <span class="c1"># Pre-allocate memory for key-values for inference.</span>
        <span class="c1"># =================================================</span>
        <span class="c1"># =====================</span>
        <span class="c1"># Query, Key, and Value</span>
        <span class="c1"># =====================</span>

        <span class="c1"># Attention heads [sq, b, h] --&gt; [sq, b, (np * 3 * hn)]</span>
        <span class="n">mixed_x_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_key_value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_attention</span><span class="p">:</span>
            <span class="p">(</span><span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">)</span> <span class="o">=</span> <span class="n">mixed_x_layer</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads_per_partition</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_multi_query_groups_per_partition</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_multi_query_groups_per_partition</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span><span class="p">,</span>
                <span class="p">],</span>
                <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">query_layer</span> <span class="o">=</span> <span class="n">query_layer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">query_layer</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads_per_partition</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">key_layer</span> <span class="o">=</span> <span class="n">key_layer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">key_layer</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_multi_query_groups_per_partition</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">value_layer</span> <span class="o">=</span> <span class="n">value_layer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">value_layer</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_multi_query_groups_per_partition</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_tensor_shape</span> <span class="o">=</span> <span class="n">mixed_x_layer</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
                               <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads_per_partition</span><span class="p">,</span>
                                <span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span><span class="p">)</span>
            <span class="n">mixed_x_layer</span> <span class="o">=</span> <span class="n">mixed_x_layer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">new_tensor_shape</span><span class="p">)</span>

            <span class="c1"># [sq, b, np, 3 * hn] --&gt; 3 [sq, b, np, hn]</span>
            <span class="p">(</span><span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">)</span> <span class="o">=</span> <span class="n">split_tensor_along_last_dim</span><span class="p">(</span><span class="n">mixed_x_layer</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="c1"># apply relative positional encoding (rotary embedding)</span>
        <span class="k">if</span> <span class="n">rotary_pos_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query_layer</span> <span class="o">=</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">query_layer</span><span class="p">,</span> <span class="n">rotary_pos_emb</span><span class="p">)</span>
            <span class="n">key_layer</span> <span class="o">=</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">key_layer</span><span class="p">,</span> <span class="n">rotary_pos_emb</span><span class="p">)</span>

        <span class="c1"># adjust key and value for inference</span>
        <span class="k">if</span> <span class="n">kv_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache_k</span><span class="p">,</span> <span class="n">cache_v</span> <span class="o">=</span> <span class="n">kv_cache</span>
            <span class="n">key_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cache_k</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">value_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cache_v</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">kv_cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">kv_cache</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_attention</span><span class="p">:</span>
            <span class="n">key_layer</span> <span class="o">=</span> <span class="n">key_layer</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">key_layer</span> <span class="o">=</span> <span class="n">key_layer</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads_per_partition</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_multi_query_groups_per_partition</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">key_layer</span> <span class="o">=</span> <span class="n">key_layer</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">key_layer</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads_per_partition</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">value_layer</span> <span class="o">=</span> <span class="n">value_layer</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">value_layer</span> <span class="o">=</span> <span class="n">value_layer</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads_per_partition</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_multi_query_groups_per_partition</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">value_layer</span> <span class="o">=</span> <span class="n">value_layer</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">value_layer</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads_per_partition</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size_per_attention_head</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># ==================================</span>
        <span class="c1"># core attention computation</span>
        <span class="c1"># ==================================</span>

        <span class="n">context_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">core_attention</span><span class="p">(</span><span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>

        <span class="c1"># =================</span>
        <span class="c1"># Output. [sq, b, h]</span>
        <span class="c1"># =================</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">context_layer</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">kv_cache</span></div>
</div>



<span class="k">def</span> <span class="nf">_config_to_kwargs</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="n">common_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">common_kwargs</span>


<div class="viewcode-block" id="MLP">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.MLP">[docs]</a>
<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;MLP.</span>

<span class="sd">    MLP will take the input with h hidden state, project it to 4*h</span>
<span class="sd">    hidden dimension, perform nonlinear transformation, and project the</span>
<span class="sd">    state back into h hidden dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ChatGLMConfig</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_bias</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">add_bias_linear</span>

        <span class="c1"># Project to 4h. If using swiglu double the output width, see https://arxiv.org/pdf/2002.05202.pdf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_h_to_4h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">ffn_hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">add_bias</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="o">**</span><span class="n">_config_to_kwargs</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">def</span> <span class="nf">swiglu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">activation_func</span> <span class="o">=</span> <span class="n">swiglu</span>

        <span class="c1"># Project back to h.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_4h_to_h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">add_bias</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="o">**</span><span class="n">_config_to_kwargs</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="p">)</span>

<div class="viewcode-block" id="MLP.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.MLP.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="c1"># [s, b, 4hp]</span>
        <span class="n">intermediate_parallel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_h_to_4h</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">intermediate_parallel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_func</span><span class="p">(</span><span class="n">intermediate_parallel</span><span class="p">)</span>
        <span class="c1"># [s, b, h]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_4h_to_h</span><span class="p">(</span><span class="n">intermediate_parallel</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span></div>
</div>



<div class="viewcode-block" id="GLMBlock">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.GLMBlock">[docs]</a>
<span class="k">class</span> <span class="nc">GLMBlock</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A single transformer layer.</span>

<span class="sd">    Transformer layer takes input with size [s, b, h] and returns an</span>
<span class="sd">    output of the same size.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ChatGLMConfig</span><span class="p">,</span> <span class="n">layer_number</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GLMBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_number</span> <span class="o">=</span> <span class="n">layer_number</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_residual_connection_post_layernorm</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">apply_residual_connection_post_layernorm</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fp32_residual_connection</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">fp32_residual_connection</span>

        <span class="n">LayerNormFunc</span> <span class="o">=</span> <span class="n">RMSNorm</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">rmsnorm</span> <span class="k">else</span> <span class="n">LayerNorm</span>
        <span class="c1"># Layernorm on the input data.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span> <span class="o">=</span> <span class="n">LayerNormFunc</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layernorm_epsilon</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                                             <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">)</span>

        <span class="c1"># Self attention.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_number</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout</span>

        <span class="c1"># Layernorm on the attention output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span> <span class="o">=</span> <span class="n">LayerNormFunc</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layernorm_epsilon</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                                                      <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">)</span>

        <span class="c1"># MLP</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<div class="viewcode-block" id="GLMBlock.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.GLMBlock.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">rotary_pos_emb</span><span class="p">,</span> <span class="n">kv_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># hidden_states: [s, b, h]</span>

        <span class="c1"># Layer norm at the beginning of the transformer layer.</span>
        <span class="n">layernorm_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># Self attention.</span>
        <span class="n">attention_output</span><span class="p">,</span> <span class="n">kv_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span>
            <span class="n">layernorm_output</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">rotary_pos_emb</span><span class="p">,</span>
            <span class="n">kv_cache</span><span class="o">=</span><span class="n">kv_cache</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span>
        <span class="p">)</span>

        <span class="c1"># Residual connection.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_residual_connection_post_layernorm</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">layernorm_output</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">layernorm_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_output</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">layernorm_input</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">layernorm_input</span>

        <span class="c1"># Layer norm post the self attention.</span>
        <span class="n">layernorm_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span><span class="p">(</span><span class="n">layernorm_input</span><span class="p">)</span>

        <span class="c1"># MLP.</span>
        <span class="n">mlp_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">layernorm_output</span><span class="p">)</span>

        <span class="c1"># Second residual connection.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_residual_connection_post_layernorm</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">layernorm_output</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">layernorm_input</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">mlp_output</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">kv_cache</span></div>
</div>



<div class="viewcode-block" id="GLMTransformer">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.GLMTransformer">[docs]</a>
<span class="k">class</span> <span class="nc">GLMTransformer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Transformer class.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ChatGLMConfig</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GLMTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fp32_residual_connection</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">fp32_residual_connection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_layer_norm</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">post_layer_norm</span>

        <span class="c1"># Number of layers.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_layers</span>

        <span class="c1"># Transformer layers.</span>
        <span class="k">def</span> <span class="nf">build_layer</span><span class="p">(</span><span class="n">layer_number</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">GLMBlock</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_number</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">build_layer</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)])</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layer_norm</span><span class="p">:</span>
            <span class="n">LayerNormFunc</span> <span class="o">=</span> <span class="n">RMSNorm</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">rmsnorm</span> <span class="k">else</span> <span class="n">LayerNorm</span>
            <span class="c1"># Final layer norm before output.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">final_layernorm</span> <span class="o">=</span> <span class="n">LayerNormFunc</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layernorm_epsilon</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                                                 <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_get_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_number</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_number</span><span class="p">]</span>

<div class="viewcode-block" id="GLMTransformer.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.GLMTransformer.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">rotary_pos_emb</span><span class="p">,</span> <span class="n">kv_caches</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">kv_caches</span><span class="p">:</span>
            <span class="n">kv_caches</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)]</span>
        <span class="n">presents</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                    <span class="s2">&quot;`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...&quot;</span>
                <span class="p">)</span>
                <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">all_self_attentions</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="n">all_hidden_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_layer</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">layer_ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span>
                    <span class="n">layer</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">rotary_pos_emb</span><span class="p">,</span>
                    <span class="n">kv_caches</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
                    <span class="n">use_cache</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">layer_ret</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">rotary_pos_emb</span><span class="p">,</span>
                    <span class="n">kv_cache</span><span class="o">=</span><span class="n">kv_caches</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
                    <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span>
                <span class="p">)</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">kv_cache</span> <span class="o">=</span> <span class="n">layer_ret</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">presents</span> <span class="o">=</span> <span class="n">presents</span> <span class="o">+</span> <span class="p">(</span><span class="n">kv_cache</span><span class="p">,)</span>

        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="n">all_hidden_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="c1"># Final layer norm.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layer_norm</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">presents</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attentions</span></div>
</div>



<div class="viewcode-block" id="ChatGLMPreTrainedModel">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMPreTrainedModel">[docs]</a>
<span class="k">class</span> <span class="nc">ChatGLMPreTrainedModel</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An abstract class to handle weights initialization and</span>
<span class="sd">    a simple interface for downloading and loading pretrained models.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">is_parallelizable</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">supports_gradient_checkpointing</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">ChatGLMConfig</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;transformer&quot;</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;GLMBlock&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the weights.&quot;&quot;&quot;</span>
        <span class="k">return</span>

<div class="viewcode-block" id="ChatGLMPreTrainedModel.get_masks">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMPreTrainedModel.get_masks">[docs]</a>
    <span class="k">def</span> <span class="nf">get_masks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">past_key_values</span><span class="p">,</span> <span class="n">padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">full_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">full_attention_mask</span><span class="o">.</span><span class="n">tril_</span><span class="p">()</span>
        <span class="n">past_length</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="n">past_length</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">past_length</span><span class="p">:</span>
            <span class="n">full_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">past_length</span><span class="p">,</span>
                                                        <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">full_attention_mask</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">full_attention_mask</span> <span class="o">=</span> <span class="n">full_attention_mask</span> <span class="o">*</span> <span class="n">padding_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">past_length</span> <span class="ow">and</span> <span class="n">padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">full_attention_mask</span> <span class="o">-=</span> <span class="n">padding_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">full_attention_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">full_attention_mask</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
        <span class="n">full_attention_mask</span><span class="o">.</span><span class="n">unsqueeze_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">full_attention_mask</span></div>


<div class="viewcode-block" id="ChatGLMPreTrainedModel.get_position_ids">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMPreTrainedModel.get_position_ids">[docs]</a>
    <span class="k">def</span> <span class="nf">get_position_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">position_ids</span></div>


    <span class="k">def</span> <span class="nf">_set_gradient_checkpointing</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">GLMTransformer</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="n">value</span></div>



<div class="viewcode-block" id="Embedding">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.Embedding">[docs]</a>
<span class="k">class</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Language model embeddings.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ChatGLMConfig</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Embedding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="c1"># Word embeddings (parallel).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">padded_vocab_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fp32_residual_connection</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">fp32_residual_connection</span>

<div class="viewcode-block" id="Embedding.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.Embedding.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
        <span class="c1"># Embeddings.</span>
        <span class="n">words_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">words_embeddings</span>
        <span class="c1"># Data format change to avoid explicit tranposes : [b s h] --&gt; [s b h].</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="c1"># If the input flag for fp32 residual connection is set, convert for float.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fp32_residual_connection</span><span class="p">:</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">embeddings</span></div>
</div>



<div class="viewcode-block" id="ChatGLMModel">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMModel">[docs]</a>
<span class="k">class</span> <span class="nc">ChatGLMModel</span><span class="p">(</span><span class="n">ChatGLMPreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ChatGLMConfig</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">empty_init</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">empty_init</span><span class="p">:</span>
            <span class="n">init_method</span> <span class="o">=</span> <span class="n">skip_init</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">init_method</span> <span class="o">=</span> <span class="n">default_init</span>
        <span class="n">init_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">init_method</span><span class="p">(</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_group_num</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">multi_query_group_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kv_channels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">kv_channels</span>

        <span class="c1"># Rotary positional embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">seq_length</span>
        <span class="n">rotary_dim</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">kv_channels</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">config</span><span class="o">.</span><span class="n">kv_channels</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_pos_emb</span> <span class="o">=</span> <span class="n">RotaryEmbedding</span><span class="p">(</span><span class="n">rotary_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">original_impl</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">original_rope</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                                              <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">init_method</span><span class="p">(</span><span class="n">GLMTransformer</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">init_method</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">padded_vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                        <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_seq_len</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pre_seq_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefix_projection</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">prefix_projection</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_seq_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pre_seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prefix_encoder</span> <span class="o">=</span> <span class="n">PrefixEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>

<div class="viewcode-block" id="ChatGLMModel.get_input_embeddings">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMModel.get_input_embeddings">[docs]</a>
    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">word_embeddings</span></div>


<div class="viewcode-block" id="ChatGLMModel.get_prompt">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMModel.get_prompt">[docs]</a>
    <span class="k">def</span> <span class="nf">get_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">):</span>
        <span class="n">prefix_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_tokens</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">past_key_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_encoder</span><span class="p">(</span><span class="n">prefix_tokens</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pre_seq_len</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_group_num</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kv_channels</span>
        <span class="p">)</span>
        <span class="c1"># seq_len, b, nh, hidden_size</span>
        <span class="n">past_key_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">)</span>
        <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="o">.</span><span class="n">permute</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">past_key_values</span></div>


<div class="viewcode-block" id="ChatGLMModel.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMModel.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">full_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_seq_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">past_key_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_prompt</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                                                  <span class="n">dtype</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_seq_len</span><span class="p">)),</span>
                                            <span class="n">attention_mask</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">full_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">all</span><span class="p">())</span> <span class="ow">or</span> <span class="p">(</span><span class="n">past_key_values</span> <span class="ow">and</span> <span class="n">seq_length</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">full_attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_masks</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">past_key_values</span><span class="p">,</span> <span class="n">padding_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

        <span class="c1"># Rotary positional embeddings</span>
        <span class="n">rotary_pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_pos_emb</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rotary_pos_emb</span> <span class="o">=</span> <span class="n">rotary_pos_emb</span><span class="p">[</span><span class="n">position_ids</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">rotary_pos_emb</span> <span class="o">=</span> <span class="n">rotary_pos_emb</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:</span><span class="n">seq_length</span><span class="p">]</span>
        <span class="n">rotary_pos_emb</span> <span class="o">=</span> <span class="n">rotary_pos_emb</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

        <span class="c1"># Run encoder.</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">presents</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attentions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
            <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">full_attention_mask</span><span class="p">,</span> <span class="n">rotary_pos_emb</span><span class="o">=</span><span class="n">rotary_pos_emb</span><span class="p">,</span>
            <span class="n">kv_caches</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">presents</span><span class="p">,</span> <span class="n">all_hidden_states</span><span class="p">,</span> <span class="n">all_self_attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">BaseModelOutputWithPast</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">presents</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">all_self_attentions</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="ChatGLMModel.quantize">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMModel.quantize">[docs]</a>
    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_bit_width</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">.quantization</span> <span class="kn">import</span> <span class="n">quantize</span>
        <span class="n">quantize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">,</span> <span class="n">weight_bit_width</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>
</div>



<div class="viewcode-block" id="ChatGLMForConditionalGeneration">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMForConditionalGeneration">[docs]</a>
<span class="k">class</span> <span class="nc">ChatGLMForConditionalGeneration</span><span class="p">(</span><span class="n">ChatGLMPreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ChatGLMConfig</span><span class="p">,</span> <span class="n">empty_init</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_sequence_length</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">ChatGLMModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">empty_init</span><span class="o">=</span><span class="n">empty_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantized</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">quantization_bit</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">quantization_bit</span><span class="p">,</span> <span class="n">empty_init</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_model_kwargs_for_generation</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">outputs</span><span class="p">:</span> <span class="n">ModelOutput</span><span class="p">,</span>
            <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
            <span class="n">is_encoder_decoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">standardize_cache_format</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="c1"># update past_key_values</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_past_from_model_output</span><span class="p">(</span>
            <span class="n">outputs</span><span class="p">,</span> <span class="n">standardize_cache_format</span><span class="o">=</span><span class="n">standardize_cache_format</span>
        <span class="p">)</span>

        <span class="c1"># update attention mask</span>
        <span class="k">if</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>

        <span class="c1"># update position ids</span>
        <span class="k">if</span> <span class="s2">&quot;position_ids&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span>
            <span class="n">new_position_id</span> <span class="o">=</span> <span class="n">position_ids</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">new_position_id</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">position_ids</span><span class="p">,</span> <span class="n">new_position_id</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>

        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;is_first_forward&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="n">model_kwargs</span>

<div class="viewcode-block" id="ChatGLMForConditionalGeneration.prepare_inputs_for_generation">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMForConditionalGeneration.prepare_inputs_for_generation">[docs]</a>
    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">is_first_forward</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># only last token for input_ids if past is not None</span>
        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_position_ids</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_first_forward</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">position_ids</span> <span class="o">=</span> <span class="n">position_ids</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;position_ids&quot;</span><span class="p">:</span> <span class="n">position_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;return_last_logit&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span>
        <span class="p">}</span></div>


<div class="viewcode-block" id="ChatGLMForConditionalGeneration.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMForConditionalGeneration.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">return_last_logit</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">transformer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">transformer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">return_last_logit</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">lm_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">lm_logits</span> <span class="o">=</span> <span class="n">lm_logits</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lm_logits</span> <span class="o">=</span> <span class="n">lm_logits</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

            <span class="c1"># Shift so that tokens &lt; n predict n</span>
            <span class="n">shift_logits</span> <span class="o">=</span> <span class="n">lm_logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">shift_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="c1"># Flatten the tokens</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">shift_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">shift_labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

            <span class="n">lm_logits</span> <span class="o">=</span> <span class="n">lm_logits</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">lm_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">transformer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">CausalLMOutputWithPast</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">lm_logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">transformer_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">transformer_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">transformer_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span></div>


    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span>
            <span class="n">past</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">...</span><span class="p">],</span> <span class="n">beam_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">...</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or</span>
<span class="sd">        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct</span>
<span class="sd">        beam_idx at every generation step.</span>

<span class="sd">        Output shares the same memory storage as `past`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">layer_past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">beam_idx</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">layer_past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">)),</span>
                <span class="n">layer_past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">beam_idx</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">layer_past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">)),</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past</span>
        <span class="p">)</span>

<div class="viewcode-block" id="ChatGLMForConditionalGeneration.process_response">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMForConditionalGeneration.process_response">[docs]</a>
    <span class="k">def</span> <span class="nf">process_response</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">history</span><span class="p">):</span>
        <span class="n">content</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="n">history</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;&lt;|assistant|&gt;&quot;</span><span class="p">):</span>
            <span class="n">metadata</span><span class="p">,</span> <span class="n">content</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">maxsplit</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">metadata</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">content</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
                <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;metadata&quot;</span><span class="p">:</span> <span class="n">metadata</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">content</span><span class="p">})</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">content</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;[[训练时间]]&quot;</span><span class="p">,</span> <span class="s2">&quot;2023年&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;metadata&quot;</span><span class="p">:</span> <span class="n">metadata</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">content</span><span class="p">})</span>
                <span class="k">if</span> <span class="n">history</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;role&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;system&quot;</span> <span class="ow">and</span> <span class="s2">&quot;tools&quot;</span> <span class="ow">in</span> <span class="n">history</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="n">content</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                    <span class="k">def</span> <span class="nf">tool_call</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">kwargs</span>
                    <span class="n">parameters</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
                    <span class="n">content</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">metadata</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="n">parameters</span><span class="p">}</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">content</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">metadata</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">content</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">content</span><span class="p">,</span> <span class="n">history</span></div>


<div class="viewcode-block" id="ChatGLMForConditionalGeneration.chat">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMForConditionalGeneration.chat">[docs]</a>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">chat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">history</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">role</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
             <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8192</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">history</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
        <span class="n">logits_processor</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">InvalidScoreLogitsProcessor</span><span class="p">())</span>
        <span class="n">gen_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="n">max_length</span><span class="p">,</span> <span class="s2">&quot;num_beams&quot;</span><span class="p">:</span> <span class="n">num_beams</span><span class="p">,</span> <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="n">do_sample</span><span class="p">,</span> <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="n">top_p</span><span class="p">,</span>
                      <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">temperature</span><span class="p">,</span> <span class="s2">&quot;logits_processor&quot;</span><span class="p">:</span> <span class="n">logits_processor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">}</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">build_chat_input</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">history</span><span class="o">=</span><span class="n">history</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">get_command</span><span class="p">(</span><span class="s2">&quot;&lt;|user|&gt;&quot;</span><span class="p">),</span>
                        <span class="n">tokenizer</span><span class="o">.</span><span class="n">get_command</span><span class="p">(</span><span class="s2">&quot;&lt;|observation|&gt;&quot;</span><span class="p">)]</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">gen_kwargs</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]):</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="n">role</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>
        <span class="n">response</span><span class="p">,</span> <span class="n">history</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_response</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">history</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span><span class="p">,</span> <span class="n">history</span></div>


<div class="viewcode-block" id="ChatGLMForConditionalGeneration.stream_chat">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMForConditionalGeneration.stream_chat">[docs]</a>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">stream_chat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">history</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">role</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
                    <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8192</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                    <span class="n">logits_processor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_past_key_values</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">history</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
        <span class="n">logits_processor</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">InvalidScoreLogitsProcessor</span><span class="p">())</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">get_command</span><span class="p">(</span><span class="s2">&quot;&lt;|user|&gt;&quot;</span><span class="p">),</span>
                        <span class="n">tokenizer</span><span class="o">.</span><span class="n">get_command</span><span class="p">(</span><span class="s2">&quot;&lt;|observation|&gt;&quot;</span><span class="p">)]</span>
        <span class="n">gen_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="n">max_length</span><span class="p">,</span> <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="n">do_sample</span><span class="p">,</span> <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="n">top_p</span><span class="p">,</span>
                      <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">temperature</span><span class="p">,</span> <span class="s2">&quot;logits_processor&quot;</span><span class="p">:</span> <span class="n">logits_processor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">build_chat_input</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">history</span><span class="o">=</span><span class="n">history</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">build_chat_input</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">past_length</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">pre_seq_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">past_length</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">pre_seq_len</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">position_ids</span> <span class="o">+=</span> <span class="n">past_length</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">attention_mask</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">past_length</span><span class="p">),</span> <span class="n">attention_mask</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">attention_mask</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="n">role</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>
        <span class="k">for</span> <span class="n">outputs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">stream_generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
                                            <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="n">return_past_key_values</span><span class="o">=</span><span class="n">return_past_key_values</span><span class="p">,</span>
                                            <span class="o">**</span><span class="n">gen_kwargs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">return_past_key_values</span><span class="p">:</span>
                <span class="n">outputs</span><span class="p">,</span> <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]):</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">response</span> <span class="ow">and</span> <span class="n">response</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;�&quot;</span><span class="p">:</span>
                <span class="n">response</span><span class="p">,</span> <span class="n">new_history</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_response</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">history</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">return_past_key_values</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="n">response</span><span class="p">,</span> <span class="n">new_history</span><span class="p">,</span> <span class="n">past_key_values</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="n">response</span><span class="p">,</span> <span class="n">new_history</span></div>


<div class="viewcode-block" id="ChatGLMForConditionalGeneration.stream_generate">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMForConditionalGeneration.stream_generate">[docs]</a>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">stream_generate</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">stopping_criteria</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StoppingCriteriaList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">prefix_allowed_tokens_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">return_past_key_values</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_ids_seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">generation_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">generation_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generation_config</span>
        <span class="n">generation_config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">generation_config</span><span class="p">)</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">bos_token_id</span><span class="p">,</span> <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">eos_token_id</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">eos_token_id</span> <span class="o">=</span> <span class="p">[</span><span class="n">eos_token_id</span><span class="p">]</span>
        <span class="n">eos_token_id_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="n">has_default_max_length</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">has_default_max_length</span> <span class="ow">and</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Using `max_length`&#39;s default (</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">) to control the generation length. &quot;</span>
                <span class="s2">&quot;This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we&quot;</span>
                <span class="s2">&quot; recommend using `max_new_tokens` to control the maximum length of the generation.&quot;</span><span class="p">,</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="o">+</span> <span class="n">input_ids_seq_length</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">has_default_max_length</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Both `max_new_tokens` (=</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="si">}</span><span class="s2">) and `max_length`(=&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">) seem to have been set. `max_new_tokens` will take precedence. &quot;</span>
                    <span class="s2">&quot;Please refer to the documentation for more information. &quot;</span>
                    <span class="s2">&quot;(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)&quot;</span><span class="p">,</span>
                    <span class="ne">UserWarning</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ids_seq_length</span> <span class="o">&gt;=</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="p">:</span>
            <span class="n">input_ids_string</span> <span class="o">=</span> <span class="s2">&quot;decoder_input_ids&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="s2">&quot;input_ids&quot;</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Input length of </span><span class="si">{</span><span class="n">input_ids_string</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">input_ids_seq_length</span><span class="si">}</span><span class="s2">, but `max_length` is set to&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">generation_config</span><span class="o">.</span><span class="n">max_length</span><span class="si">}</span><span class="s2">. This can lead to unexpected behavior. You should consider&quot;</span>
                <span class="s2">&quot; increasing `max_new_tokens`.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># 2. Set generation parameters if not already defined</span>
        <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">logits_processor</span> <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
        <span class="n">stopping_criteria</span> <span class="o">=</span> <span class="n">stopping_criteria</span> <span class="k">if</span> <span class="n">stopping_criteria</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">StoppingCriteriaList</span><span class="p">()</span>

        <span class="n">logits_processor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_processor</span><span class="p">(</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">input_ids_seq_length</span><span class="o">=</span><span class="n">input_ids_seq_length</span><span class="p">,</span>
            <span class="n">encoder_input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
            <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">stopping_criteria</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stopping_criteria</span><span class="p">(</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span> <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">stopping_criteria</span>
        <span class="p">)</span>
        <span class="n">logits_warper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_warper</span><span class="p">(</span><span class="n">generation_config</span><span class="p">)</span>

        <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
            <span class="c1"># forward pass to get next token</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># pre-process distribution</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_logits</span><span class="p">)</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">logits_warper</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_scores</span><span class="p">)</span>

            <span class="c1"># sample</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">do_sample</span><span class="p">:</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># update generated ids, model inputs, and length for next step</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">,</span> <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
            <span class="p">)</span>
            <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">unfinished_sequences</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span>
                <span class="n">next_tokens</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">eos_token_id_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">eos_token_id_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">return_past_key_values</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">input_ids</span>
            <span class="c1"># stop when each sentence is finished, or if we exceed the maximum length</span>
            <span class="k">if</span> <span class="n">unfinished_sequences</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">stopping_criteria</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">):</span>
                <span class="k">break</span></div>


<div class="viewcode-block" id="ChatGLMForConditionalGeneration.quantize">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMForConditionalGeneration.quantize">[docs]</a>
    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bits</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">empty_init</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">bits</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="kn">from</span> <span class="nn">.quantization</span> <span class="kn">import</span> <span class="n">quantize</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantized</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Already quantized.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">quantized</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="n">bits</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">encoder</span><span class="p">,</span> <span class="n">bits</span><span class="p">,</span> <span class="n">empty_init</span><span class="o">=</span><span class="n">empty_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                                            <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>
</div>



<div class="viewcode-block" id="ChatGLMForSequenceClassification">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMForSequenceClassification">[docs]</a>
<span class="k">class</span> <span class="nc">ChatGLMForSequenceClassification</span><span class="p">(</span><span class="n">ChatGLMPreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ChatGLMConfig</span><span class="p">,</span> <span class="n">empty_init</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">ChatGLMModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">empty_init</span><span class="o">=</span><span class="n">empty_init</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">classifier_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">classifier_dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">classifier_dropout</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">quantization_bit</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">quantization_bit</span><span class="p">,</span> <span class="n">empty_init</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<div class="viewcode-block" id="ChatGLMForSequenceClassification.forward">
<a class="viewcode-back" href="../../../diffsynth.models.html#diffsynth.models.kolors_text_encoder.ChatGLMForSequenceClassification.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">full_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">SequenceClassifierOutputWithPast</span><span class="p">]:</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">transformer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">full_attention_mask</span><span class="o">=</span><span class="n">full_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">transformer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">pooled_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pooled_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">pooled_hidden_states</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier_head</span><span class="p">(</span><span class="n">pooled_hidden_states</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;regression&quot;</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span> <span class="ow">or</span> <span class="n">labels</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;single_label_classification&quot;</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;multi_label_classification&quot;</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span><span class="p">:</span>
                <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">labels</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;single_label_classification&quot;</span><span class="p">:</span>
                <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">==</span> <span class="s2">&quot;multi_label_classification&quot;</span><span class="p">:</span>
                <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">transformer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">SequenceClassifierOutputWithPast</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">transformer_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">transformer_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">transformer_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, ModelScope.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>